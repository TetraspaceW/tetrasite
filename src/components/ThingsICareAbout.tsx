export const ThingsICareAboutSection = () => {
  return (
    <>
      <h2>Things I Care About</h2>
      <p>
        I am a big fan of rationalism and{" "}
        <a href="./effective-altruism.html">effective altruism</a>. I think at
        some point this century, the
        <a href="./cybersingularity.html">cybersingularity</a> will occur, in
        which AIs become smarter than people in the non-metaphorical
        impossible-to-miss way that results in things like planets being
        dismantled, and that we are currently not on track for this to happen in
        a way that leaves survivors.
      </p>
      <p>
        The most effective way for an individual donor to give is through a
        <a href="https://www.givingwhatwecan.org/donor-lottery">
          donor lottery
        </a>
        . Giving What We Can runs one once a year. The most important
        object-level issue, and probably where we want money to flow because
        money can be exchanged for goods and services and we want the goods and
        services, is
        <a href="https://twitter.com/ESYudkowsky/status/1570967796582076416?s=20&t=WJZfWfq_lsPqHcButM4cXA">
          AI notkilleveryoneism
        </a>
        , and the situation for AI safety is currently looking dire;
        <strong>
          please read
          <a href="https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities">
            AGI Ruin: A List of Lethalities
          </a>
        </strong>
        for a view of the awful state our civilisation is in on this issue.
      </p>
      <p>
        For the AI safety funding situation, Scott Alexander
        <a href="https://astralcodexten.substack.com/p/so-you-want-to-run-a-microgrants">
          wrote on Feb 9th 2022
        </a>
        :
      </p>
      <blockquote>
        (what actually happened was that the Long Term Future Fund approached me
        and said “we will fund every single good AI-related proposal you get,
        just hand them to us, you don't have to worry about it”. Then I had
        another person say “hand me the ones Long Term Future Fund doesn't want,
        and I'll fund those.” Have I mentioned it's a good time to start AI
        related charities?)
      </blockquote>
      <p>
        The collapse of FTX no doubt made the expected funding situation worse,
        previously being a third of the net present value of committed EA funds,
        though Open Philanthropy is still going strong.
        <em>
          A good funding situation does not mean that civilisation has AI
          notkilleveryoneism handled
        </em>
        ; what it does mean is that if you join the civilisational effort
        against everyone dying from AI, it might be possible for you to be
        funded.
      </p>
    </>
  );
};
